# I was provided with a dataset of audio recordings and their phoneme state labels from articles published in the Wall Street Journal that are read aloud and labelled using the original text. There were 40 phonemes considered in this case. The speech data is stored as a sequence of numbers that represent the amplitude of the sound wave at each time step. It is composed of sound waves of several different frequencies overlaid on top of one another. The data was transformed to melspectrogram each representing 25ms of speech data. I trained this model with a MLP model with a reverse cylindrical shape. One thing I encountered was that the since each melspectrogram (which is a matrix) only represents 25ms of speech, it does not accurately represent a phoneme. This is solved by adding context and feed several melspectrogram in batch into the network. I achieved 85.7% accuracy at last. 
